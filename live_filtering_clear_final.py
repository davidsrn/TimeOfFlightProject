# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import Voxel
import sys
# sys.path.insert(0, "/usr/lib/python2.7/Voxel.py")
# import Voxel.py
# import threading
import numpy as np
from matplotlib import pyplot as plt
import time
# import imutils
import os

import tensorflow as tf
import tiny_face_model
import util
from argparse import ArgumentParser
import cv2
import scipy.io
import numpy as np
import matplotlib.pyplot as plt
import pickle

import pylab as pl
from scipy.special import expit
import glob

prev = []
font = cv2.FONT_HERSHEY_SIMPLEX

MAX_INPUT_DIM = 5000.0

f=lambda a: (abs(a)+a)/2

global graph1

graph = tf.Graph()

back_imgs = 50
drop_imgs = 10

def overlay_bounding_boxes(raw_img, refined_bboxes, lw):
    """Overlay bounding boxes of face on images.
    Args:
      raw_img:
        A target image.
      refined_bboxes:
        Bounding boxes of detected faces.
      lw:
        Line width of bounding boxes. If zero specified,
        this is determined based on confidence of each detection.
    Returns:
      None.
    """
    boxes_w_color = []
    # Overlay bounding boxes on an image with the color based on the confidence.
    for r in refined_bboxes:
        _score = expit(r[4])
        cm_idx = int(np.ceil(_score * 255))
        rect_color = [int(np.ceil(x * 255)) for x in util.cm_data[cm_idx]]  # parula
        _lw = lw
        if lw == 0:  # line width of each bounding box is adaptively determined.
            bw, bh = r[2] - r[0] + 1, r[3] - r[0] + 1
            _lw = 1 if min(bw, bh) <= 20 else max(2, min(3, min(bh / 20, bw / 20)))
            _lw = int(np.ceil(_lw * _score))

        _r = [int(x) for x in r[:4]]

        boxes_w_color.append([_r[0], _r[1], _r[2], _r[3], rect_color])

    return np.array(boxes_w_color)
    # cv2.rectangle(raw_img, (_r[0], _r[1]), (_r[2], _r[3]), rect_color, _lw)


def evaluate(raw_img,clusters, clusters_h, clusters_w, normal_idx, average_image, score_final, model, x, prob_thresh=0.5, nms_thresh=0.1, lw=3, display=False):
    """Detect faces in images.
    Args:
      prob_thresh:
          The threshold of detection confidence.
      nms_thresh:
          The overlap threshold of non maximum suppression
      weight_file_path:
          A pretrained weight file in the pickle format
          generated by matconvnet_hr101_to_tf.py.
      data_dir:
          A directory which contains images.
      output_dir:
          A directory into which images with detected faces are output.
      lw:
          Line width of bounding boxes. If zero specified,
          this is determined based on confidence of each detection.
      display:
          Display tiny face images on window.
    Returns:
      None.
    """
    # main
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        # A partir de aquÃ­, empieza a tratar individualmente el input
        # Aqui separa la ruta, y obtiene solamente el nombre del archivo
        # No es necesario para arreglos npy
        #
        # Ahora tengo que implementar nuevamente el para que recorra todo raw_img
        # fname = filename.split(os.sep)[-1]
        #raw_img = cv2.imread(filename)
        # raw_img = np.load(filename)
        # Convierte una imagen de un espacio de color a otro
        #raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)
        # raw_img = raw_img*255/raw_img.max()

        imgs = []
        for i in range(3):
            new_image = raw_img
            imgs.append(new_image)
        imgs = np.stack(imgs, axis=2)
        #print("asd: ", imgs.shape)
        #
        #raw_img_f = raw_img.astype(np.float32)

        def _calc_scales():
            raw_h, raw_w = imgs.shape[0], imgs.shape[1]
            #print("raw_h: ",imgs.shape[0])
            min_scale = min(np.floor(np.log2(np.max(clusters_w[normal_idx] / raw_w))),np.floor(np.log2(np.max(clusters_h[normal_idx] / raw_h))))
            max_scale = min(1.0, -np.log2(max(raw_h, raw_w) / MAX_INPUT_DIM))
            scales_down = pl.frange(min_scale, 0, 1.)
            scales_up = pl.frange(0.5, max_scale, 0.5)
            scales_pow = np.hstack((scales_down, scales_up))
            scales = np.power(2.0, scales_pow)
            return scales

        scales = _calc_scales()
        start = time.time()

        # initialize output
        bboxes = np.empty(shape=(0, 5))

        iter = 2
        # process input at different scales
        for s in scales:
            # print("SCALES")
            # print("-------------------------------------")
            # print(s, iter)
            # print("-------------------------------------")
            iter+=1
            if iter%2==0 or iter ==3:
                continue
            # Aqui voy a cambiar porqye fname ya no es  necesario fname se cambiara por raw_img
            print("Processing {} at scale {:.4f}", s)
            img = cv2.resize(imgs, (0, 0), fx=s, fy=s, interpolation=cv2.INTER_LINEAR)
            #print("img: ", img.shape,"\navg: ",  average_image.shape)
            img = img - average_image
            img = img[np.newaxis, :]

            # we don't run every template on every scale ids of templates to ignore
            tids = list(range(4, 12)) + ([] if s <= 1.0 else list(range(18, 25)))
            ignoredTids = list(set(range(0, clusters.shape[0])) - set(tids))

            # run through the net
            score_final_tf = sess.run(score_final, feed_dict={x: img})

            # collect scores
            score_cls_tf, score_reg_tf = score_final_tf[:, :, :, :25], score_final_tf[:, :, :, 25:125]
            prob_cls_tf = expit(score_cls_tf)
            prob_cls_tf[0, :, :, ignoredTids] = 0.0

            def _calc_bounding_boxes():
                # threshold for detection
                _, fy, fx, fc = np.where(prob_cls_tf > prob_thresh)

                # interpret heatmap into bounding boxes
                cy = fy * 8 - 1
                cx = fx * 8 - 1
                ch = clusters[fc, 3] - clusters[fc, 1] + 1
                cw = clusters[fc, 2] - clusters[fc, 0] + 1

                # extract bounding box refinement
                Nt = clusters.shape[0]
                tx = score_reg_tf[0, :, :, 0:Nt]
                ty = score_reg_tf[0, :, :, Nt:2*Nt]
                tw = score_reg_tf[0, :, :, 2*Nt:3*Nt]
                th = score_reg_tf[0, :, :, 3*Nt:4*Nt]

                # refine bounding boxes
                dcx = cw * tx[fy, fx, fc]
                dcy = ch * ty[fy, fx, fc]
                rcx = cx + dcx
                rcy = cy + dcy
                rcw = cw * np.exp(tw[fy, fx, fc])
                rch = ch * np.exp(th[fy, fx, fc])

                scores = score_cls_tf[0, fy, fx, fc]
                tmp_bboxes = np.vstack((rcx - rcw / 2, rcy - rch / 2, rcx + rcw / 2, rcy + rch / 2))
                tmp_bboxes = np.vstack((tmp_bboxes / s, scores))
                tmp_bboxes = tmp_bboxes.transpose()
                return tmp_bboxes

            tmp_bboxes = _calc_bounding_boxes()
            bboxes = np.vstack((bboxes, tmp_bboxes)) # <class 'tuple'>: (5265, 5)


        # print("time {:.2f} secs for {}".format(time.time() - start, "frame"))

        # non maximum suppression
        # refind_idx = util.nms(bboxes, nms_thresh)
        refind_idx = tf.image.non_max_suppression(tf.convert_to_tensor(bboxes[:, :4], dtype=tf.float32),tf.convert_to_tensor(bboxes[:, 4], dtype=tf.float32),max_output_size=bboxes.shape[0], iou_threshold=nms_thresh)
        refind_idx = sess.run(refind_idx)
        refined_bboxes = bboxes[refind_idx]
        #print(type(refined_bboxes))
        # np.save("coords.npy", refined_bboxes)
        refined_bboxes = overlay_bounding_boxes(raw_img, refined_bboxes, lw)
    # sess.close()
    return refined_bboxes

def imshow_components(labels):
    # Map component labels to hue val
    label_hue = np.uint8(179*labels/np.max(labels))
    blank_ch = 255*np.ones_like(label_hue)
    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])
    # cvt to BGR for display
    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)
    # set bg label to black
    labeled_img[label_hue==0] = 0
    # cv2.imshow('labeled.png', labeled_img)
    # cv2.waitKey()
    return labeled_img

def createWindow():
    a = np.array([])
    global window
    if window == None:
        window = MainWindow(cameraSystem)
    return

class MainWindow():

    def __init__(self, cameraSystem):
        print("MainWindow init")

        # Define el modelo con los pesos que le pasamos
        # Create the tiny face model which weights are loaded from a pretrained model.
        self.graph1 = tf.Graph()
        with self.graph1.as_default():
            self.x = tf.placeholder(tf.float32, [1, None, None, 3]) # n, h, w, c
            self.model = tiny_face_model.Model("./out2")
            print(self.model)
            self.score_final = self.model.tiny_face(self.x)

        # Aqui esta el primer cambio que tengo que hacer
        # Lo que hace es crear un arreglo vacio que luego llenara con un for
        # Find image files in data_dir.
        self.raw_img = []
        # print(filename)

        # Carga el pickle
        # Load an average image and clusters(reference boxes of templates).
        with open("./out2", "rb") as f:
          _, self.mat_params_dict = pickle.load(f)

        # Cosas del modelo
        self.average_image = self.model.get_data_by_key("average_image")
        self.clusters = self.model.get_data_by_key("clusters")
        self.clusters_h = self.clusters[:, 3] - self.clusters[:, 1] + 1
        self.clusters_w = self.clusters[:, 2] - self.clusters[:, 0] + 1
        self.normal_idx = np.where(self.clusters[:, 4] == 1)

        self.depthCamera = cameraSystem.connect(devices[0])
        self.data = {}
        # self.flag = 100
        self.coords=[]
        self.flag = 0
        self.prev = []
        self.drop = 0
        self.elip_kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(2,4))
        # self.maxv = 0
        # self.minv = 0
        self.background = np.zeros((240, 320))
        self.kernel = np.ones((5,5),np.uint8)
        self.small_kernel = np.ones((2,2),np.uint8)
        if self.depthCamera:
            self.depthCamera.clearAllCallbacks()
            self.depthCamera.registerCallback(Voxel.DepthCamera.FRAME_DEPTH_FRAME, self.processDepthFrame)
            if not self.depthCamera.start():
                print(" start ok")
            else:
                print("Camera started with a framerate of: ", self.depthCamera.getFrameRate()[1].denominator)

    def processDepthFrame(self, depthCamera, frame, type):

        if frame is None:
            return

        depthFrame = Voxel.DepthFrame.typeCast(frame)
        if not depthFrame:
            return
        if self.drop < drop_imgs:
            self.drop = self.drop + 1
        elif self.flag < back_imgs and depthFrame:
            # self.background = self.background+np.array(depthFrame.depth).reshape((depthFrame.size.height, depthFrame.size.width))
            back = (np.array(depthFrame.depth).reshape((depthFrame.size.height, depthFrame.size.width)))
            back[back>8] = 8

            self.background = self.background+cv2.GaussianBlur(back,(7,7),0)

            self.flag = self.flag + 1
            # print(self.flag)
            if self.flag == back_imgs:
                self.flag = self.flag + 1
                print("\n\nFinished reading background")
                self.background = self.background / back_imgs
                cv2.imwrite("background_raw.jpg", ((f(self.background))*255/(self.background.max())).astype(np.uint8))


        else:
            amplitude_array = np.array(depthFrame.amplitude)
            amp_res = amplitude_matrix = amplitude_array.reshape((depthFrame.size.height, depthFrame.size.width))
            #
            # print("--------------------")
            #
            # print(amp_res.max())
            # print("--------------------")
            # amp_res = amp_res * 255 / 0.09874922
            0.03564453
            amp_res[amp_res>0.025] = 0.025

            amp_res = amp_res * 255 / amp_res.max()

            amp_res = amp_res.astype(np.uint8)

            depth_array = np.array(depthFrame.depth)
            dep_res = depth_matrix = depth_array.reshape((depthFrame.size.height, depthFrame.size.width))
            cv2.imwrite("phase_raw.jpg", ((f(dep_res))*255/(dep_res.max())).astype(np.uint8))
            dep_res[dep_res>8] = 8

            dep_res = (self.background - cv2.GaussianBlur(depth_matrix, (7,7), 0))
            # dep_res = (self.background -depth_matrix)
            cv2.imwrite("phase_bkg_substraction.jpg", ((f(dep_res))*255/(dep_res.max())).astype(np.uint8))

            dep_res = (cv2.morphologyEx((cv2.morphologyEx(dep_res, cv2.MORPH_OPEN, self.kernel)), cv2.MORPH_CLOSE, self.kernel))
            # dep_res = cv2.erode(dep_res, self.small_kernel)
            dep_res = ((f(dep_res))*255/(dep_res.max())).astype(np.uint8)
            cv2.imwrite("phase_prosseced.jpg", dep_res)

            # np.save("./images/frame.npy", amp_res)

            # os.system('python tiny_face_eval_single.py --weight_file_path ./out2 --data_dir ./images/frame.npy --output_dir ./test')

            with self.graph1.as_default():
            #
                faces = evaluate(amp_res, self.clusters, self.clusters_h, self.clusters_w, self.normal_idx, self.average_image, self.score_final, self.model, self.x)

            # faces = np.array([])

            dep_res[dep_res<40] = 0
            thresh = cv2.adaptiveThreshold(dep_res, 255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,3,2)
            cv2.imwrite("threshold_raw.jpg", thresh)

            """
                The following line is to make the outline of the person thicker, this works when there's not alot of noise inside of the detected person
                For low ambient scenarios its recomended to uncomment it.
            """

            # thresh = (cv2.morphologyEx((cv2.morphologyEx(thresh, cv2.MORPH_OPEN, self.kernel)), cv2.MORPH_CLOSE, self.kernel))


            """
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            """

            # thresh =
            # thresh = cv2.dilate(cv2.erode(thresh, self.elip_kernel,  iterations = 2), self.elip_kernel,  iterations = 1)
            # thresh = cv2.erode(thresh, self.elip_kernel,  iterations = 1)
            zeros = thresh == 0

            dep_res[zeros] = thresh[zeros]


            # thresh = (cv2.GaussianBlur(thresh, (5,5), 0))
            # _,thresh = cv2.threshold(dep_res,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            # thresh = cv2.morphologyEx(cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, self.kernel), cv2.MORPH_CLOSE, self.kernel)
            cv2.imwrite("threshold.jpg", thresh)
            # cv2.imwrite("threshold_clean.jpg", thresh)
            # cv2.imwrite("threshold_dilate.jpg", thresh)

            # gradient = cv2.morphologyEx(thresh, cv2.MORPH_GRADIENT, self.kernel)
            # cv2.imwrite("gradient.jpg", gradient)
            # contours = cv2.findContours(thresh)
            #
            # for cnt in contours:
            #     # area = cv2.contourArea(cnt)
            #     print(cnt)
            #     # x,y,w,h = cv2.boundingRect(cnt)
            #     # thresh = cv2.rectangle(thresh,(x,y),(x+w,y+h),(0,255,0),2)
            #     # cv2.imwrite("threshold_w_contours.jpg", thresh)

            _, conn, rects, centroids = cv2.connectedComponentsWithStats(image=dep_res, connectivity=8, ltype=cv2.CV_16U)
            # print(a)
            cv2.imwrite("connected.jpg", imshow_components(conn))


            person = 0
            persons = []
            # persons = rects
            # print("Faces:")
            # print(faces)
            # print("Blobs:")
            # print(rects)


            for rect in rects:
                if rect[4] > 500 and rect[4] < 17000:
                    persons.append(rect)
                    # persons = np.setdiff1d(rects, rect)
                    # print(rect)




            # rects = np.array([rects])
            # current = []
            # if len(self.prev)==0:
            #     if(len(persons)==0):
            #         self.prev = []
            #     else:
            #         self.prev = persons
            #         # self.prev.append(persons)
            # else:
            #     if(len(persons)==0):
            #         self.prev = []
            #     else:
            #         print(self.prev)
            #         for rect in self.prev:
            #             for per_rect in persons:
            #                 # print(rect[0])
            #                 # # print(self.prev)
            #                 # print(per_rect[0])
            #                 if abs(rect[0]- per_rect[0]) < 20 and abs(rect[1] - per_rect[1]) < 20:
            #                     current.append(per_rect)
            #                 else:
            #                     current.append(rect)
            #         self.prev = current

            persons = np.array(persons)
            # print("Person blobs:")
            # print(persons)
            if persons.size != 0:
                for rect in persons:
                    # print("Rectangle:")
                    # print(rect)
                    for face in faces:
                        # print("Face:")0
                        # print(face)
                        if (rect[0]-20 < face[0]) and (rect[1]-10 < face[1]) and ((rect[0] + rect[2]) > face[2]) and ((rect[1] + rect[3] + 5) > face[3]):
                            person = person + 1
                            break

            dep_res_rgb = cv2.cvtColor(dep_res, cv2.COLOR_GRAY2RGB)
            if faces.size != 0:
                for data in faces:
                    # print(data[4])
                    cv2.rectangle(dep_res_rgb, (int(data[0]), int(data[1])), (int(data[2]), int(data[3])), data[4], 2)
                    # cv2.rectangle(amp_res, (_r[0], _r[1]), (_r[2], _r[3]), rect_color, _lw)

            if persons.size != 0:
                for data in persons:
                    cv2.rectangle(dep_res_rgb, (int(data[0]), int(data[1])), (int(data[0]+data[2]), int(data[1]+data[3])), (0, 0, 255), 2)
                    # print(data)
            # cv2.putText(dep_res, ("Bodies: " + str(len(persons))),(0, 60), font, 1,(255,255,255),2,cv2.LINE_AA)
            # cv2.putText(dep_res, ("Faces: " + str(len(faces))),(0, 120), font, 1,(255,255,255),2,cv2.LINE_AA)
            cv2.putText(dep_res_rgb, ("Persons: " + str(person)),(0, 60), font, 1,(255,255,255),2,cv2.LINE_AA)
            # cv2.imshow("Persons", dep_res_rgb)
            # cv2.imshow("Persons = " + str(person), dep_res_rgb)
            cv2.imwrite("phase_with_detection.jpg", dep_res_rgb)
            cv2.imwrite("amplitude.jpg", amp_res)
	    cv2.imshow("Persons", dep_res_rgb)
            cv2.waitKey(10) & 0XFF
            # print(self.coords)

    def stop(self):
        if self.depthCamera:
            self.depthCamera.stop()
            self.depthCamera.clearAllCallbacks()
            del self.depthCamera
            self.depthCamera = None


cameraSystem = Voxel.CameraSystem()

devices = cameraSystem.scan()

if len(devices) == 1:
    print(" Find one device.")
    window = MainWindow(cameraSystem)
    key = raw_input("Input enter key to quit.")
    print(" Quit now.")
    window.stop()
else:
    print(" No device found.")

del cameraSystem
